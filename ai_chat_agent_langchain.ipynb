{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries and stuff\n",
    "#pip install -U langchain --user\n",
    "#pip install openai==0.28 #embedding doesn't work for recent version????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot agent using OpenAI and LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('squad', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0]['answers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0]['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0]['context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df['context'].duplicated()) #check duplicates, rerun after next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset='context', keep='first', inplace=True) #remove duplicates of context, keeping ONE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = 'openai key'\n",
    "\n",
    "model = 'text-embedding-ada-002' #commonly used embedding model\n",
    "\n",
    "res = openai.Embedding.create(input = 'I love OpenAI', engine = model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_vector = res['data'][0]['embedding'] #parsing embedded vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model):\n",
    "    text = text.replace('\\n', ' ')\n",
    "    res = openai.Embedding.create(input = text, engine = model)\n",
    "    return res['data'][0]['embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = get_embedding('Testing the embedding', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vec) #save this, we need the length to adjust the dimension in Pinecone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = 'pinecone key'\n",
    "env = 'pinecone env'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, PodSpec\n",
    "from tqdm.autonotebook import tqdm #progress bar\n",
    "pc = Pinecone(api_key=api_key)\n",
    "#delete data in old index (delete the actual index in the Pinecone client)\n",
    "pc.create_index(name='ai-agent',dimension=len(vec),metric='dotproduct',spec=PodSpec(\n",
    "    environment=env,\n",
    "    pod_type= \"Starter\",\n",
    "    pods= 1\n",
    "  ))\n",
    "idx = pc.Index('ai-agent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df.sample(2500, random_state=45)\n",
    "batch_size = 10\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%use to compute waiting time\n",
    "for i in tqdm(range(0, len(df_sample), batch_size)):\n",
    "    i_end = min(i+batch_size, len(df_sample))\n",
    "    batch = df_sample.iloc[i:i_end]\n",
    "    metadata = [{\"title\": row['title'], \"context\": row['context']} for i, row in batch.iterrows()] \n",
    "    #for the embedding, we use the openai api\n",
    "    docs = batch['context'].tolist() #pd-series to python list\n",
    "    emb_vectors = [get_embedding(doc, model) for doc in docs]\n",
    "\n",
    "    ids = batch['id'].tolist()\n",
    "\n",
    "    to_upsert = zip(ids,emb_vectors,metadata)\n",
    "    idx.upsert(vectors=to_upsert)\n",
    "\n",
    "    time.sleep(15) #use to avoid overloading, meaning that you aren't billed from openai\n",
    "    #basically, you want an iteration of batch_size=10 to take 1 minute in total\n",
    "    #subtract the wall time from 60 to find your computers time to sleep\n",
    "    #ONLY WORKS FOR SMALL LOADS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding2(text):\n",
    "    text = text.replace('\\n', ' ')\n",
    "    res = openai.Embedding.create(input = text, engine = 'text-embedding-ada-002')\n",
    "    return res['data'][0]['embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Pinecone(idx, get_embedding2, 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'What does the state of obesity contribute to'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pure semantic, no generation\n",
    "vectorstore.similarity_search(query, k=3)\n",
    "#may not receive a proper answer with 100 records, try increasing it if you want\n",
    "#may also be completely wrong somewhere, don't want to debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "#from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "openai_key = 'openai key'\n",
    "model_name = 'text-embedding-ada-002'\n",
    "embed = OpenAIEmbeddings(\n",
    "    model = model_name,\n",
    "    openai_api_key = openai_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embed.embed_query(\"embedding single query\") #embedding single document\n",
    "\n",
    "embed.embed_documents([\"first doc\", \"second doc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clear pinecone database before running this cell\n",
    "for i in tqdm(range(0, len(df_sample), batch_size)):\n",
    "    i_end = min(i+batch_size, len(df_sample))\n",
    "    batch = df_sample.iloc[i:i_end]\n",
    "    metadata = [{\"title\": row['title'], \"context\": row['context']} for i, row in batch.iterrows()] \n",
    "    #for the embedding, we use the openai api\n",
    "    docs = batch['context'].tolist() #pd-series to python list\n",
    "    emb_vectors = embed.embed_documents(docs) #use this instead of having to define our own embedding model\n",
    "    #either one works, langchain is nice to use so we don't have to write our own functions which can contain bugs\n",
    "    ids = batch['id'].tolist()\n",
    "\n",
    "    to_upsert = zip(ids,emb_vectors,metadata)\n",
    "    idx.upsert(vectors=to_upsert)\n",
    "\n",
    "    #time.sleep(15) #use to avoid overloading, meaning that you aren't billed from openai\n",
    "    #basically, you want an iteration of batch_size=10 to take 1 minute in total\n",
    "    #subtract the wall time from 60 to find your computers time to sleep\n",
    "    #ONLY WORKS FOR SMALL LOADS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define AI Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llmodel = ChatOpenAI(openai_api_key=openai_key, model_name = 'gpt-3.5-turbo', temperature = 0.0)\n",
    " #model trained up to september 2021, temp to get conservative responses\n",
    "\n",
    "#define conversational memory\n",
    "conv_mem = ConversationBufferWindowMemory(memory_key='chat_history', k=5, return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=llmodel, chain_type='stuff', retriever = vectorstore.as_retriever())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key difference here between using an agent and semantic search is simply that we get the answer straight away using an agent. Semantic search instead gives the documents where we can find the answers for ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "tools = [\n",
    "    Tool(\n",
    "        name = 'Knowledge Base',\n",
    "        func = qa.run,\n",
    "        description = 'Use this when answering based on knowledge'\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = initialize_agent(\n",
    "    agent = AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,\n",
    "    tools = tools,\n",
    "    llm = llmodel,\n",
    "    verbose = True,\n",
    "    max_iterations = 3,\n",
    "    early_stopping_method = 'generate',\n",
    "    memory = conv_mem\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent('What does obesity contribute to')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code seems to be faulty somewhere, doesn't return any response from the vector database. Might do something about it later on."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
